{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Combine the three different files to form a single file, having entries from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "impcat=pd.read_csv('C:/Users/hp/Desktop/JFM20/IM ML OJT/Week 1/Task 3/impcat_data_june2020.csv')\n",
    "pdp=pd.read_csv('C:/Users/hp/Desktop/JFM20/IM ML OJT/Week 1/Task 3/pdp_data_june2020.csv')\n",
    "ga=pd.read_csv('C:/Users/hp/Desktop/JFM20/IM ML OJT/Week 1/Task 3/ga_data_new_sept20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impcat.append(pdp, sort=False)\n",
    "result = df.append(ga, sort=False)\n",
    "print(result.columns)\n",
    "fin=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Keep only the mentioned columns: ['Query','Impressions','Avg_Position','GLCAT_MCAT_ID','GLCAT_MCAT_NAME','Total Unique Searches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result[['Query','Impressions','Avg_Position','GLCAT_MCAT_ID','GLCAT_MCAT_NAME','Total Unique Searches']]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For each query in a MCAT: \n",
    "   a.  If only one entry for a query exists (in a MCAT) calculate and store the following as:\n",
    "       data[impression]=data[‘Impressions’]\n",
    "       data[unique_searches]= data[‘Unique Searches’]\n",
    "       data[avg_position]= data['Avg_Position']\n",
    "   b.If more than one entry for a query exists (in a MCAT) calculate and store the following as:\n",
    "     data[impression]= Sum of Impressions\n",
    "     data[unique_searches]= Sum of Unique Searches\n",
    "     data[avg_position]= Average of 'Avg_Position'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tup=tuple(result.groupby(['GLCAT_MCAT_ID','Query']))\n",
    "#dictionary = {} \n",
    "pairs=dict(tuple(result.groupby(['GLCAT_MCAT_ID','Query'])))\n",
    "#group= dict((x, y) for x, y in tup)\n",
    "#group= dictionary(Convert(tup, dictionary))\n",
    "print (len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=0\n",
    "for x,y in pairs.items():\n",
    "    print (x)\n",
    "    y=y.reset_index(drop=True)\n",
    "    mcat_name=str(y['GLCAT_MCAT_NAME'][0])\n",
    "    y['Impressions']=y['Impressions'].fillna(0)\n",
    "    y['Avg_Position']=y['Avg_Position'].fillna(0)\n",
    "    y['Total Unique Searches']=y['Total Unique Searches'].fillna(0)\n",
    "    mcat_id=int(x[0])\n",
    "    query=str(x[1])\n",
    "    if (len(y)==1):\n",
    "        sum_impression=y['Impressions'][0]\n",
    "        sum_average_position=y['Avg_Position'][0]\n",
    "        sum_unique_searches=y['Total Unique Searches'][0]\n",
    "    else:\n",
    "        sum_impression=sum(y['Impressions'])\n",
    "        impression_list=list(y['Impressions'])\n",
    "        average_position_list=list(y['Avg_Position'])\n",
    "        #try:\n",
    "        if (sum_impression!=0):\n",
    "            sum_average_position = np.average(average_position_list, weights=impression_list)\n",
    "        else:\n",
    "            sum_average_position=0\n",
    "        sum_unique_searches=sum(y['Total Unique Searches'])\n",
    "    fin.at[r,\"MCAT_ID\"]=mcat_id\n",
    "    fin.at[r,\"MCAT_NAME\"]=mcat_name\n",
    "    fin.at[r,\"Query\"]=query\n",
    "    fin.at[r,\"IMPRESSIONS\"]=sum_impression\n",
    "    fin.at[r,\"AVG POSITION\"]=sum_average_position\n",
    "    fin.at[r,\"UNIQUE_SEARCHES\"]=sum_unique_searches\n",
    "    r+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Columns after this step: mcat_id, mcat_name, query,impression, avg_position, unique_searches\n",
    "2. Save the results in a file (combined_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.to_csv('C:/Users/hp/Desktop/combined_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query, we need to clean it by applying the following steps:\n",
    "○\tConversion to lowercase\n",
    "○\tKeeping only alphabets and numeric entries\n",
    "○\tRemoval of all english stop words\n",
    "○\tRemoval of certain keywords mentioned below:\n",
    "[\"manufacturer\",\"manufacture\",\"manufacturing\",\"manufactured\",\"price\",\"wholesaler\",\"wholesalers\",\"wholesale\",\"deal\",\"dealer\",\"deals\",\"dealers\",\"distribution\",\"supplier\",\"distributor\",\"distributors\",\"india\",\"rate\",\"cost\",\"costs\",\"near me\",\"buy\",\"buys\",\"online\",\"company\",  \"exporter\",\"exporters\",\"good\",\"topmost\",\"business\",\"trusted\",\"finest\",\"offer\",\"organization\",\"trader\",\"organizations\",\"traders\",\"offers\",\"indiamart\",\"delhi\",\"manufacturers\",\"prices\",\"suppliers\",\"manufactures\"]\n",
    "\n",
    "○\tRemoval of location names provided in the pickle file (location.pkl)\n",
    "○\tSingularize the query\n",
    "○\tReplace one than one spaces with a single space\n",
    "○\tTrim the extra spaces (trailing and preceding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re \n",
    "import pickle \n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "from pattern.en import singularize\n",
    "from nltk.corpus import stopwords\n",
    "all_stopwords = stopwords.words('english')\n",
    "\n",
    "sw_1=[\"manufacturer\",\"manufacture\",\"manufacturing\",\"manufactured\",\"price\",\"wholesaler\",\"wholesalers\",\"wholesale\",\"deal\",\"dealer\",\"deals\",\"dealers\",\"distribution\",\"supplier\",\"distributor\",\"distributors\",\"india\",\"rate\",\"cost\",\"costs\",\"near me\",\"buy\",\"buys\",\"online\",\"company\",\n",
    "      \"exporter\",\"exporters\",\"good\",\"topmost\",\"business\",\"trusted\",\"finest\",\"offer\",\"organization\",\"trader\",\"organizations\",\"traders\",\"offers\",\"indiamart\",\"delhi\",\"manufacturers\",\"prices\",\"suppliers\",\"manufactures\"]\n",
    "\n",
    "location = list(pickle.load(open(r'C:\\Users\\hp\\Desktop\\location_set.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=str(text)\n",
    "    text=text.lower()\n",
    "    x=re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "    x=x.strip()\n",
    "    tokens_sw1 = ' '.join(word for word in  w_tokenizer.tokenize(x) if not word in all_stopwords)\n",
    "    tokens_sw2 = ' '.join(word for word in  w_tokenizer.tokenize(tokens_sw1) if not word in sw_1)\n",
    "    tokens_sw3 = ' '.join(word for word in  w_tokenizer.tokenize(tokens_sw2) if not word in location)\n",
    "    stem_text=' '.join(singularize(plural) for plural in w_tokenizer.tokenize(tokens_sw3))\n",
    "    tokens_sw4 = ' '.join(word for word in  w_tokenizer.tokenize(stem_text) if not word in sw_1)\n",
    "    x1 = re.sub(' +', ' ',tokens_sw4)\n",
    "    x1=x1.strip()\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give us a clean_query corresponding to each query in a MCAT.\n",
    "Columns after this step: mcat_id, mcat_name, query, impression, avg_position, unique_searches, query_cleaned\n",
    "Save the results in a file (cleaned_queries.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"Query_Cleaned\"]=result[\"Query\"].apply(clean)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(r'C:\\Users\\hp\\Desktop\\cleaned_queries.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each clean_query in a MCAT (please note that many queries when cleaned might result in the same clean_query), we will calculate and store the following:\n",
    "○\tdata[‘Impressions’]= Sum of Impressions\n",
    "○\tdata[‘unique_search’]= Sum of Unique Searches\n",
    "○\tdata[‘frequency’]= Count of the times ‘clean_query’ appears in a MCAT\n",
    "○\tIf the clean_query exists as it is in the original data, we take the value of “avg_position” for our clean_query, else we take the Minimum of average_position\n",
    "○\tFor a MCAT, sort the values as per ‘Impressions’ of queries.(high to low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=dict(tuple(result.groupby(['MCAT_ID'])))\n",
    "print (len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1_1=pd.DataFrame()\n",
    "for key,value in groups.items():\n",
    "    result_1=pd.DataFrame()\n",
    "    row=0\n",
    "    print (key)\n",
    "    value=value.reset_index()\n",
    "    mcat_name=str(value['MCAT_NAME'][0])\n",
    "    queries=list(set(value['Query_Cleaned']))\n",
    "    for i in range(0,len(queries)):\n",
    "        print (queries[i])\n",
    "        df1=value[value['Query_Cleaned']==queries[i]]\n",
    "        df1=df1.reset_index()\n",
    "        df1['IMPRESSIONS'] = df1['IMPRESSIONS'].fillna(0)\n",
    "        df1['UNIQUE_SEARCHES'] = df1['UNIQUE_SEARCHES'].fillna(0)\n",
    "        sum_impression=sum(df1['IMPRESSIONS'])\n",
    "        sum_searches=sum(df1['UNIQUE_SEARCHES'])\n",
    "        \n",
    "        rank=df1[df1['Query']==queries[i]]\n",
    "        rank=rank.reset_index(drop=True)\n",
    "        if (len(rank)>=1):\n",
    "            pos=rank['AVG POSITION'][0]\n",
    "        else:\n",
    "            pos_list=list(df1['AVG POSITION'])\n",
    "            try:\n",
    "                pos=min(i for i in pos_list if i!=0)\n",
    "            except:\n",
    "                pos=0\n",
    "        \n",
    "        del (rank)\n",
    "        freq=len(df1)\n",
    "        result_1.at[row,\"MCAT_ID\"]=int(key)\n",
    "        result_1.at[row,\"MCAT_NAME\"]=mcat_name\n",
    "        result_1.at[row,\"Query\"]=str(queries[i])\n",
    "        result_1.at[row,\"FREQUENCY\"]=int(freq)\n",
    "        result_1.at[row,\"SUM_IMPRESSIONS\"]=sum_impression\n",
    "        result_1.at[row,\"POSITION\"]=pos\n",
    "        result_1.at[row,\"SUM_UNIQUE_SEARCHES\"]=sum_searches\n",
    "        row=row+1\n",
    "    result_1=result_1.sort_values(by=['SUM_IMPRESSIONS'],ascending=False)\n",
    "    result_1=result_1[result[\"Query\"]!=\"\"]\n",
    "    result_1=result_1.reset_index()\n",
    "    result1_1=result1_1.append(result_1)\n",
    "    print(result1_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns after this step: mcat_id, mcat_name, query (cleaned_query), frequency, sum_impression, position, sum_unique_searches\n",
    "Save the results in a file (cleaned_queries_with_freq.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1_1.to_csv(r'C:\\Users\\hp\\Desktop\\cleaned_queries_with_freq.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each MCAT:\n",
    "○\tFind all the uni-grams.\n",
    "○\tFor each uni-gram, calculate the total sum_impressions (summation of “sum_impression” values for every entry where the uni-gram exists)\n",
    "○\tReturn top-15 for every MCAT. (as per “sum_impression”- high to low)\n",
    "○\tSave the results in a file (MCAT_wise_uni_grams.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unigrams\n",
    "result1_1=pd.DataFrame()\n",
    "for key,value1 in groups.items():\n",
    "    Tokens = []\n",
    "    Unique_List=list(value1['Query_Cleaned'].astype(str))\n",
    "    for i in range(len(Unique_List)):\n",
    "        temp_w = Unique_List[i].split(' ')\n",
    "        Tokens = Tokens + temp_w\n",
    "        tokens_without_sw = [word for word in Tokens if not word in all_stopwords]\n",
    "    DG_df = pd.DataFrame(data=tokens_without_sw,columns=['KW'])\n",
    "    DG_df=DG_df[DG_df[\"KW\"]!='']\n",
    "    uni_grams=list(DG_df[\"KW\"])\n",
    "    for i in range(0,len(uni_grams)):\n",
    "        value1[\"Query1\"]=\" \"+value1[\"Query_Cleaned\"].astype(str)+\" \"\n",
    "        l=\" \"+str(uni_grams[i])+\" \"\n",
    "        a=value1[value1[\"Query1\"].str.contains(l)]\n",
    "        a=a.reset_index()\n",
    "        a[\"IMPRESSIONS\"]=a[\"IMPRESSIONS\"].fillna(0)\n",
    "        sum_impression=sum(a[\"IMPRESSIONS\"])\n",
    "        DG_df.loc[DG_df.KW == uni_grams[i], 'Sum_Impressions'] = sum_impression\n",
    "    DG_df[\"MCAT_ID\"]=int(key)\n",
    "    DG_df=DG_df.sort_values(by=['Sum_Impressions'],ascending=False)\n",
    "    DG_df=DG_df.drop_duplicates()\n",
    "    DG_df=DG_df.reset_index()\n",
    "    DG_df=DG_df.iloc[:15]\n",
    "    result1_1=result1_1.append(DG_df)\n",
    "    print (result1_1.shape)\n",
    "result1_1.to_csv(r'C:\\Users\\hp\\Desktop\\MCAT_wise_uni_grams.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "○Find all the bi-grams.\n",
    "○For each bi-gram, calculate the total sum_impressions (summation of “sum_impression” values for every entry where the bi-gram exists)\n",
    "○Return top-15 for every MCAT. (as per “sum_impression”- high to low)\n",
    "○Save the results in a file (MCAT_wise_bi_grams.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams\n",
    "import pandas as pd\n",
    "result1_1=pd.DataFrame()\n",
    "for key,value1 in groups.items():\n",
    "    Tokens = []\n",
    "    Unique_List=list(value1['Query_Cleaned'].astype(str))\n",
    "    for i in range(len(Unique_List)):\n",
    "        temp_w = Unique_List[i].split(' ')\n",
    "        temp_w_1=[word for word in temp_w if not word in all_stopwords]\n",
    "        if(len(temp_w_1) >=2):\n",
    "            for j in range(len(temp_w_1)-1):\n",
    "                w1 = str(temp_w_1[j]) + \" \" + str(temp_w_1[j+1])\n",
    "                Tokens = Tokens + [w1]\n",
    "    DG_df = pd.DataFrame(data=Tokens,columns=['KW'])\n",
    "    bi_grams=list(DG_df[\"KW\"])\n",
    "    for i in range(0,len(bi_grams)):\n",
    "        value1[\"Query1\"]=\" \"+value1[\"Query_Cleaned\"].astype(str)+\" \"\n",
    "        l=\" \"+str(bi_grams[i])+\" \"\n",
    "        a=value1[value1[\"Query1\"].str.contains(l)]\n",
    "        a=a.reset_index()\n",
    "        a[\"IMPRESSIONS\"]=a[\"IMPRESSIONS\"].fillna(0)\n",
    "        sum_impression=sum(a[\"IMPRESSIONS\"])\n",
    "        DG_df.loc[DG_df.KW == bi_grams[i], 'Sum_Impressions'] = sum_impression\n",
    "    DG_df[\"MCAT_ID\"]=int(key)\n",
    "    DG_df=DG_df.sort_values(by=['Sum_Impressions'],ascending=False)\n",
    "    DG_df=DG_df.drop_duplicates()\n",
    "    DG_df=DG_df.reset_index()\n",
    "    # top 15\n",
    "    DG_df=DG_df.iloc[:15]\n",
    "    result1_1=result1_1.append(DG_df)\n",
    "    print (result1_1.shape)\n",
    "result1_1.to_csv(r'C:\\Users\\hp\\Desktop\\MCAT_wise_bi_grams.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
